{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7AmNEZgRgvNztJOjHtiot",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedSamirScience/Prediction-of-adaptor-proteins-using-PSSM-profiles-and-Recurrent-Neural-Networks.-/blob/main/PredictionOfAdaptorAndNonAdaptorProteinFinalVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuMN3mvc6p0u"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "import glob\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBM9WATe7lUl",
        "outputId": "2871d955-2c68-4c65-d204-901eaee43e3c"
      },
      "source": [
        "drive.mount('/content/gdrive/')\n",
        "path = \"gdrive/MyDrive/proteinsproject\"\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErA5WlFR9K60",
        "outputId": "dcd1c39a-4a96-4acd-cefd-2b102cb47218"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/proteinsproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "305HKHgN7Ex_"
      },
      "source": [
        "split_seed = 53\n",
        "MODEL_DIR = 'model'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "            \n",
        "file_model = \"adaptor.model\"\n",
        "\n",
        "CONV1D_FEATURE_SIZE = 256 # So bo loc\n",
        "CONV1D_KERNEL_SIZE = 3 # Kich thuoc bo loc\n",
        "AVGPOOL1D_KERNEL_SIZE = 3 # Kich thuoc bo loc trung binh\n",
        "GRU_HIDDEN_SIZE = 256\n",
        "FULLY_CONNECTED_LAYER_SIZE = 512\n",
        "\n",
        "NUMBER_EPOCHS = 30\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "LOSS_WEIGHT_POSITIVE = 10.07\n",
        "LOSS_WEIGHT_NEGATIVE = 1.11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkwz9Qiw7LKN"
      },
      "source": [
        "def load_text_file(file_text):\n",
        "    start_time = time.time()\n",
        "    with open(file_text) as f:\n",
        "        lines = f.readlines()\n",
        "    return lines\n",
        "\n",
        "def load_lst_file(file_lst, dir_name):\n",
        "    lst_file = load_text_file(file_lst)\n",
        "    lst_path = [dir_name + file_name.strip() for file_name in lst_file]\n",
        "    return lst_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpGxquO59YXC"
      },
      "source": [
        "def create_list_train():\n",
        "    lst_path_positive_train = glob.glob(\"./datasettrain/positive/*.pssm\") \n",
        "    lst_path_negative_train = glob.glob(\"./datasettrain/negative/*.pssm\")     \n",
        "    \n",
        "    print(\"Positive train: \", len(lst_path_positive_train))\n",
        "    print(\"Negative train: \", len(lst_path_negative_train))          \n",
        "    \n",
        "    lst_positive_train_label = [1] *  len(lst_path_positive_train)\n",
        "    lst_negative_train_label = [0] *  len(lst_path_negative_train)\n",
        "    \n",
        "    lst_path_train = lst_path_positive_train + lst_path_negative_train\n",
        "    lst_label_train = lst_positive_train_label + lst_negative_train_label\n",
        "    \n",
        "    print(\"Train all: \", len(lst_path_train))\n",
        "    print(\"\")\n",
        "    return lst_path_train, lst_label_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDQ7C4QgGj7a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15n_bipr9trw"
      },
      "source": [
        "class BioinformaticsDataset(Dataset):\n",
        "    # X: list cac file (full path)\n",
        "    # Y: list label [0, 1]; 0: negative, 1: positive\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.Y[index]\n",
        "        lines = load_text_file(self.X[index])\n",
        "        start_line = 3\n",
        "        end_line = len(lines) - 7\n",
        "        values = np.zeros((end_line - start_line + 1, 20))\n",
        "\n",
        "        for i in range(start_line, end_line + 1):\n",
        "            #print i\n",
        "            strs = lines[i].strip().split()[2:22]\n",
        "            #print strs\n",
        "            for j in range(20):\n",
        "                values[i-start_line][j] = int(strs[j])\n",
        "                #print(values[i-start_line][j])\n",
        "            #break\n",
        "\n",
        "        #print(\"values: \", values)\n",
        "        input = torch.from_numpy(values)\n",
        "        return input, label\n",
        "\n",
        "    def __len__(self):\n",
        "        #return 100\n",
        "        return len(self.X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMucuZ1JDOJw"
      },
      "source": [
        "\n",
        "def weighted_binary_cross_entropy(output, target, weights=True):        \n",
        "    if weights :        \n",
        "        loss = LOSS_WEIGHT_POSITIVE * (target * torch.log(output)) + \\\n",
        "               LOSS_WEIGHT_NEGATIVE * ((1 - target) * torch.log(1 - output))\n",
        "    else:\n",
        "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
        "\n",
        "    return torch.neg(torch.mean(loss))\n",
        "    \n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, n_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.c1 = nn.Conv1d(20, CONV1D_FEATURE_SIZE, CONV1D_KERNEL_SIZE)\n",
        "        self.p1 = nn.AvgPool1d(AVGPOOL1D_KERNEL_SIZE)\n",
        "        self.c2 = nn.Conv1d(CONV1D_FEATURE_SIZE, CONV1D_FEATURE_SIZE, CONV1D_KERNEL_SIZE)\n",
        "        self.p2 = nn.AvgPool1d(AVGPOOL1D_KERNEL_SIZE)\n",
        "        self.gru = nn.GRU(CONV1D_FEATURE_SIZE, GRU_HIDDEN_SIZE, n_layers, dropout=0.01)\n",
        "        self.fc = nn.Linear(GRU_HIDDEN_SIZE, FULLY_CONNECTED_LAYER_SIZE)\n",
        "        self.fc_drop = nn.Dropout()\n",
        "        self.out = nn.Linear(FULLY_CONNECTED_LAYER_SIZE, 1)\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "        self.gru_layers = n_layers\n",
        "        \n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        #print \"inputs size: \", inputs.size()\n",
        "        #print \"batch_size: \", batch_size\n",
        "        #######################\n",
        "        #  USE GPU FOR MODEL  #\n",
        "        #######################\n",
        "        if torch.cuda.is_available():\n",
        "            h0 = Variable(torch.zeros(self.gru_layers, inputs.size(0), GRU_HIDDEN_SIZE).cuda())\n",
        "        else:\n",
        "            h0 = Variable(torch.zeros(self.gru_layers, inputs.size(0), GRU_HIDDEN_SIZE))\n",
        "        #print h0.size()\n",
        "\n",
        "        # Turn (batch_size x seq_len) into (batch_size x input_size x seq_len) for CNN\n",
        "        inputs = inputs.transpose(1,2)\n",
        "        #print \"inputs size: \", inputs.size()\n",
        "        c = self.c1(inputs)\n",
        "        #print \"c1 : \", c.size()\n",
        "        p = self.p1(c)\n",
        "        #print \"p1: \", p.size()\n",
        "        c = self.c2(p)\n",
        "        #print \"c2: \", c.size()\n",
        "        p = self.p2(c)\n",
        "        #print \"p2: \", p.size()\n",
        "\n",
        "        # Turn (batch_size x hidden_size x seq_len) back into (seq_len x batch_size x hidden_size) for RNN\n",
        "        p = p.transpose(1, 2).transpose(0, 1)\n",
        "\n",
        "        #p = F.tanh(p)\n",
        "        p = F.relu(p)\n",
        "        #print \"p: \", p.size()\n",
        "\n",
        "        #output, hidden = self.gru(p, hidden)\n",
        "        output, hidden = self.gru(p, h0)\n",
        "        #conv_seq_len = output.size(0)\n",
        "        #print \"gru output: \", output.size()\n",
        "        #print \"conv_seq_len: \", conv_seq_len\n",
        "        #print \"batch_size: \", batch_size\n",
        "        #print \"GRU_HIDDEN_SIZE: \", GRU_HIDDEN_SIZE\n",
        "\n",
        "        #output = output.view(conv_seq_len * batch_size, GRU_HIDDEN_SIZE)\n",
        "        #print \"last gru output: \", output.size()\n",
        "        #output = torch.gather(output, 1, torch.tensor(30))\n",
        "        #print \"last gru output: \", output.size()\n",
        "\n",
        "        #output = F.tanh(self.out(output))\n",
        "        #print(\"Hidden: \", hidden.size())\n",
        "\n",
        "        output = F.relu(self.fc(hidden))\n",
        "        #print(\"output fc: \", output.size())\n",
        "        output = self.out(output)\n",
        "        #print(\"output out: \", output.size())\n",
        "        output = self.out_act(output)\n",
        "        #print(\"output sigmoid: \", output.size())\n",
        "        #output = output.view(conv_seq_len, -1, NUMBER_CLASSES)\n",
        "\n",
        "        return output\n",
        "        \n",
        "    def train_one_epoch(self, learning_rate):\n",
        "        self.train()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate\n",
        "\n",
        "        epoch_loss_train = 0.0\n",
        "        nb_train = 0\n",
        "        nb_train_short = 0\n",
        "        for i, data in enumerate(self.train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs_length = inputs.size()[1]\n",
        "            if inputs_length < 20:\n",
        "                nb_train_short += 1\n",
        "                continue\n",
        "\n",
        "            #continue\n",
        "\n",
        "            inputs = inputs.float()\n",
        "            labels = labels.float()\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                inputs = Variable(inputs.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "            else:\n",
        "                inputs = Variable(inputs)\n",
        "                labels = Variable(labels)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # pass to get output/logits\n",
        "            outputs = self(inputs)\n",
        "            outputs = outputs[-1,-1]\n",
        "            loss = weighted_binary_cross_entropy(outputs, labels)\n",
        "            #print(\"loss: \", loss.data[0])\n",
        "            #loss = torch.mul(loss, weight)\n",
        "\n",
        "            # Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "\n",
        "            # Updating parameters\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss_train = epoch_loss_train + loss.item()\n",
        "            nb_train = nb_train + 1\n",
        "\n",
        "            #if i > 20: break\n",
        "            #if i % 10 == 0:\n",
        "            #    print(\"Loss: \", i, \" : \", epoch_loss_train/nb_train)\n",
        "\n",
        "        print(\"nb train short (length < 20): \", nb_train_short)\n",
        "\n",
        "        epoch_loss_train_avg = epoch_loss_train / nb_train\n",
        "        return epoch_loss_train_avg\n",
        "        \n",
        "    def train_validate(self, train_dataset, validate_dataset, fold):\n",
        "        self.train_loader = DataLoader(dataset=train_dataset, batch_size=1,                              \n",
        "                              shuffle=True, num_workers=4)\n",
        "        self.val_loader = DataLoader(dataset=validate_dataset, batch_size=1,                              \n",
        "                              shuffle=True, num_workers=4)                                        \n",
        "        \n",
        "        #file_model = \"adaptor_model_{}_{}.pfl\".format(fold, best_epoch)\n",
        "        file_model = \"best_model_saved.pkl\"\n",
        "        \n",
        "        train_losses = []\n",
        "        train_lst_acc = []\n",
        "        train_lst_sensitivity = []\n",
        "        train_lst_specificity = []\n",
        "        train_lst_mcc = []\n",
        "        train_lst_auc = []\n",
        "          \n",
        "        val_losses = []\n",
        "        val_lst_acc = []\n",
        "        val_lst_sensitivity = []\n",
        "        val_lst_specificity = []\n",
        "        val_lst_mcc = []\n",
        "        val_lst_auc = []\n",
        "\n",
        "        best_val_loss = 1000\n",
        "        best_epoch = 0\n",
        "        for epoch in range(NUMBER_EPOCHS):\n",
        "            print(\"\\n############### EPOCH : \", epoch)\n",
        "            epoch_loss_train_avg = self.train_one_epoch(LEARNING_RATE)\n",
        "            train_losses.append(epoch_loss_train_avg)\n",
        "\n",
        "            print(\"\\nEvaluate Train set\")\n",
        "            result = self.evaluate(self.train_loader)\n",
        "            print(\"\\nTrain loss: \", result['epoch_loss_avg'])\n",
        "            print(\"Train acc: \", result['acc'])\n",
        "            print(\"Train confusion matrix: \\n\", result['confusion_matrix'])\n",
        "            print(\"Train sensitivity: \", result['sensitivity'])\n",
        "            print(\"Train specificity: \", result['specificity'])\n",
        "            print(\"Train mcc: \", result['mcc'])\n",
        "            print(\"Train auc: \", result['auc'])\n",
        "            \n",
        "            train_lst_acc.append(result['acc'])\n",
        "            train_lst_sensitivity.append(result['sensitivity'])\n",
        "            train_lst_specificity.append(result['specificity'])\n",
        "            train_lst_mcc.append(result['mcc'])\n",
        "            train_lst_auc.append(result['auc'])\n",
        "            \n",
        "            print(\"\\nEvaluate Val set\")\n",
        "            result_val = self.evaluate(self.val_loader)\n",
        "            print(\"\\nVal loss: \", result_val['epoch_loss_avg'])\n",
        "            print(\"Val acc: \", result_val['acc'])\n",
        "            print(\"Val confusion matrix: \\n\", result_val['confusion_matrix'])\n",
        "            print(\"Val sensitivity: \", result_val['sensitivity'])\n",
        "            print(\"Val specificity: \", result_val['specificity'])\n",
        "            print(\"Val mcc: \", result_val['mcc'])\n",
        "            print(\"Val auc: \", result_val['auc'])\n",
        "            \n",
        "            val_losses.append(result_val['epoch_loss_avg'])\n",
        "            val_lst_acc.append(result_val['acc'])\n",
        "            val_lst_sensitivity.append(result_val['sensitivity'])\n",
        "            val_lst_specificity.append(result_val['specificity'])\n",
        "            val_lst_mcc.append(result_val['mcc'])\n",
        "            val_lst_auc.append(result_val['auc'])\n",
        "            \n",
        "            if best_val_loss > result_val['epoch_loss_avg']:\n",
        "                torch.save(self.state_dict(), os.path.join(MODEL_DIR, file_model))\n",
        "                best_val_loss = result_val['epoch_loss_avg']\n",
        "                print(\"Save model, best_val_loss: \", best_val_loss)\n",
        "                best_epoch = epoch \n",
        "\n",
        "        print(\"\\ntrain_losses: \", train_losses)\n",
        "        print(\"train_lst_acc: \", train_lst_acc)\n",
        "        print(\"train_lst_sensitivity: \", train_lst_sensitivity)\n",
        "        print(\"train_lst_specificity: \", train_lst_specificity)\n",
        "        print(\"train_lst_mcc: \", train_lst_mcc)\n",
        "        print(\"train_lst_auc: \", train_lst_auc)\n",
        "        \n",
        "        print(\"\\nval_losses: \", val_losses)\n",
        "        print(\"val_lst_acc: \", val_lst_acc)\n",
        "        print(\"val_lst_sensitivity: \", val_lst_sensitivity)\n",
        "        print(\"val_lst_specificity: \", val_lst_specificity)\n",
        "        print(\"val_lst_mcc: \", val_lst_mcc)\n",
        "        print(\"val_lst_auc: \", val_lst_auc)\n",
        "        \n",
        "        #load again current_model_save for next training\n",
        "        self.load_state_dict(torch.load(MODEL_DIR+\"/best_model_saved.pkl\"))\n",
        "        #Luu lai model tot nhat ung voi fold validation hien tai\n",
        "        model_fn = \"adaptor_{}_{}.pkl\".format(fold, best_epoch)\n",
        "        torch.save(self.state_dict(), os.path.join(MODEL_DIR, model_fn))\n",
        "        print(model_fn, \"saved\")\n",
        "        \n",
        "        #Save train_loss & val_loss\n",
        "        with open(MODEL_DIR+\"/logfile_loss_model_{}_{}.csv\".format(fold, best_epoch), mode = 'w') as lf_loss:\n",
        "            lf_loss = csv.writer(lf_loss, delimiter=',')\n",
        "            lf_loss.writerow(['epoch', 'train loss', 'train acc', 'train_sens', \n",
        "                'train_spec', 'train_mcc', 'train_auc', \n",
        "                'val loss', 'val acc', 'val_sens', 'val_spec', 'val_mcc', 'val_auc'])\n",
        "            for i in range(np.size(train_losses)):                \n",
        "                lf_loss.writerow([i, train_losses[i], train_lst_acc[i],\n",
        "                    train_lst_sensitivity[i], train_lst_specificity[i],\n",
        "                    train_lst_mcc[i], train_lst_auc[i],\n",
        "                    val_losses[i], val_lst_acc[i], val_lst_sensitivity[i],\n",
        "                    val_lst_specificity[i], val_lst_mcc[i], val_lst_auc[i]])\n",
        "\n",
        "    def evaluate(self, loader):\n",
        "        self.eval()\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        nb = 0\n",
        "        nb_short = 0\n",
        "\n",
        "        arr_labels = []\n",
        "        arr_labels_hyp = []\n",
        "        arr_prob = []\n",
        "        \n",
        "        for i, data in enumerate(loader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            #print \"labels: \", labels\n",
        "            # print(\"inputs: \", inputs.size())\n",
        "            inputs_length = inputs.size()[1]\n",
        "            # print(\"inputs_length: \", inputs_length)\n",
        "            if inputs_length < 20:\n",
        "                nb_short += 1\n",
        "                continue\n",
        "\n",
        "            #print \"labels: \", labels.cpu().numpy()[0]\n",
        "            arr_labels.append(labels.cpu().numpy()[0])\n",
        "\n",
        "            inputs = inputs.float()\n",
        "            labels = labels.float()\n",
        "\n",
        "            # print \"\\nBatch i : \", i\n",
        "            # print \"inputs: \", inputs\n",
        "            # print \"labels: \", labels\n",
        "\n",
        "            #######################\n",
        "            #  USE GPU FOR MODEL  #\n",
        "            #######################\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = Variable(inputs.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs = Variable(inputs)\n",
        "                labels = Variable(labels)\n",
        "\n",
        "            # print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)\n",
        "\n",
        "            # Clear gradients w.r.t. parameters\n",
        "            #optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get output/logits\n",
        "            outputs = self(inputs)\n",
        "            outputs = outputs[-1, -1]\n",
        "\n",
        "            # loss = criterion(outputs[-1], labels)\n",
        "            # print(\"outputs: \", outputs.size())\n",
        "            # print(\"labels: \", labels.size())\n",
        "            # print(outputs)\n",
        "            # print(labels)\n",
        "            #loss = criterion(outputs, labels)\n",
        "            loss = weighted_binary_cross_entropy(outputs, labels)\n",
        "\n",
        "            #print(\"outputs: \", outputs)\n",
        "            arr_prob.append(outputs.data.cpu().numpy()[0])\n",
        "            if outputs.data.cpu().numpy()[0] > 0.5: arr_labels_hyp.append(1)\n",
        "            else: arr_labels_hyp.append(0)\n",
        "\n",
        "            #epoch_loss = epoch_loss + loss.data[0]\n",
        "            epoch_loss = epoch_loss + loss.item()\n",
        "            nb = nb + 1\n",
        "\n",
        "            #if i > 20: break\n",
        "            #if i % 10 == 0:\n",
        "            #    print(\"Loss: \", i, \" : \", epoch_loss/nb)\n",
        "\n",
        "        print(\"nb short (length < 20): \", nb_short)\n",
        "\n",
        "        #print \"arr_labels: \", arr_labels\n",
        "        #print \"arr_labels_hyp: \", arr_labels_hyp\n",
        "        epoch_loss_avg = epoch_loss / nb\n",
        "        \n",
        "        auc = metrics.roc_auc_score(arr_labels, arr_prob)\n",
        "        #print(\"auc: \", auc)\n",
        "\n",
        "        acc, confusion_matrix, sensitivity, specificity, mcc = self.calculate_confusion_matrix(arr_labels, arr_labels_hyp)\n",
        "        result = {'epoch_loss_avg': epoch_loss_avg, \n",
        "                    'acc' : acc, \n",
        "                    'confusion_matrix' : confusion_matrix,\n",
        "                    'sensitivity' : sensitivity,\n",
        "                    'specificity' : specificity,\n",
        "                    'mcc' : mcc,\n",
        "                    'auc' : auc,\n",
        "                    'arr_prob': arr_prob,\n",
        "                    'arr_labels': arr_labels\n",
        "                     }\n",
        "        return result\n",
        "    \n",
        "    def calculate_accuracy(self, arr_labels, arr_labels_hyp):\n",
        "        corrects = 0\n",
        "\n",
        "        for i in range(len(arr_labels)):\n",
        "            if arr_labels[i] == arr_labels_hyp[i]:\n",
        "                corrects = corrects + 1\n",
        "\n",
        "        acc = corrects * 1.0 / len(arr_labels)\n",
        "        return acc\n",
        "\n",
        "    def calculate_confusion_matrix(self, arr_labels, arr_labels_hyp):\n",
        "        corrects = 0\n",
        "        confusion_matrix = np.zeros((2, 2))\n",
        "\n",
        "        for i in range(len(arr_labels)):\n",
        "            confusion_matrix[arr_labels_hyp[i]][arr_labels[i]] += 1\n",
        "\n",
        "            if arr_labels[i] == arr_labels_hyp[i]:\n",
        "                corrects = corrects + 1\n",
        "\n",
        "        acc = corrects * 1.0 / len(arr_labels)\n",
        "        specificity = confusion_matrix[0][0] / (confusion_matrix[0][0] + confusion_matrix[1][0])\n",
        "        sensitivity = confusion_matrix[1][1] / (confusion_matrix[0][1] + confusion_matrix[1][1])\n",
        "        tp = confusion_matrix[1][1]\n",
        "        tn = confusion_matrix[0][0]\n",
        "        fp = confusion_matrix[1][0]\n",
        "        fn = confusion_matrix[0][1]\n",
        "        mcc = (tp * tn - fp * fn ) / math.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))\n",
        "        #print(\"mcc: \", mcc)\n",
        "        return acc, confusion_matrix, sensitivity, specificity, mcc\n",
        "        \n",
        "def training():\n",
        "    print(\"split_seed: \", split_seed)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=split_seed)\n",
        "    #Split TRAIN_VAL dataset into 5-fold, 4 for training and 1 for cross validation\n",
        "    fold = 0\n",
        "    for train_index, val_index in skf.split(lst_path_train_all, lst_label_train_all):\n",
        "        #get train and crossvalidation data by splitted indexes\n",
        "        print(\"train_index: \", train_index)\n",
        "        print(\"val_index: \", val_index)\n",
        "        X_train = [lst_path_train_all[i] for i in train_index]\n",
        "        X_val = [lst_path_train_all[i] for i in val_index]\n",
        "        Y_train = [lst_label_train_all[i] for i in train_index]\n",
        "        Y_val = [lst_label_train_all[i] for i in val_index]\n",
        "        \n",
        "        #print(\"X_train indices: \", train_index)\n",
        "        #print(\"X_val indices: \", val_index)\n",
        "        print(\"Y_train len: \", len(Y_train), \"Y_val len:\", len(Y_val))\n",
        "        unique, count = np.unique(Y_train, return_counts = True)\n",
        "        print(\"Y_train values: \", unique, \"count: \", count)\n",
        "        unique, count = np.unique(Y_val, return_counts = True)\n",
        "        print(\"y_val values: \", unique, \"count: \", count)\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        train_set = BioinformaticsDataset(X_train, Y_train)\n",
        "        val_set = BioinformaticsDataset(X_val, Y_val)\n",
        "        \n",
        "        model = RNNModel(2)\n",
        "        print(model)\n",
        "        if torch.cuda.is_available():\n",
        "            model.cuda()\n",
        "            \n",
        "        model.train_validate(train_set, val_set, fold)\n",
        "        \n",
        "        fold += 1\n",
        "        # #break\n",
        "\n",
        "    \n",
        "############### MAIN PROGRAM ##############\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Adaptor Training K Fold\\n\")\n",
        "    lst_path_train_all, lst_label_train_all = create_list_train()\n",
        "        \n",
        "    training()\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYcyFkvH_zgx"
      },
      "source": [
        "def create_list_test(): \n",
        "    lst_path_positive_test = glob.glob(\"./datasettest/positive/*.pssm\") \n",
        "    lst_path_negative_test = glob.glob(\"./datasettest/negative/*.pssm\")       \n",
        "    \n",
        "    print(\"Positive test: \", len(lst_path_positive_test))\n",
        "    print(\"Negative test: \", len(lst_path_negative_test))\n",
        "    \n",
        "    lst_positive_test_label = [1] *  len(lst_path_positive_test)\n",
        "    lst_negative_test_label = [0] *  len(lst_path_negative_test)\n",
        "    lst_path_test_all = lst_path_positive_test + lst_path_negative_test\n",
        "    lst_label_test_all = lst_positive_test_label + lst_negative_test_label\n",
        "    \n",
        "    print(\"Test all: \", len(lst_path_test_all))\n",
        "    print(\"\")\n",
        "    return lst_path_test_all, lst_label_test_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCi49W6UpMXm"
      },
      "source": [
        "def test():\n",
        "    test_set = BioinformaticsDataset(lst_path_test_all, lst_label_test_all)\n",
        "    test_loader = DataLoader(dataset=test_set, batch_size=1,                              \n",
        "                              shuffle=True, num_workers=4)\n",
        "    model = RNNModel(2)\n",
        "    print(model)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.load_state_dict(torch.load(MODEL_DIR+\"/best_model_saved.pkl\"))\n",
        "    result = model.evaluate(test_loader)\n",
        "    print(\"\\nTest loss: \", result['epoch_loss_avg'])\n",
        "    print(\"Test acc: \", result['acc'])\n",
        "    print(\"Test confusion matrix: \\n\", result['confusion_matrix'])\n",
        "    print(\"Test sensitivity: \", result['sensitivity'])\n",
        "    print(\"Test specificity: \", result['specificity'])\n",
        "    print(\"Test mcc: \", result['mcc'])\n",
        "    print(\"Test auc: \", result['auc'])\n",
        "\n",
        "def test_all():\n",
        "    test_set = BioinformaticsDataset(lst_path_test_all, lst_label_test_all)\n",
        "    test_loader = DataLoader(dataset=test_set, batch_size=1,                              \n",
        "                              shuffle=True, num_workers=4)\n",
        "    model = RNNModel(2)\n",
        "    print(model)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    \n",
        "    testresult_fn = \"/test_result.csv\"\n",
        "    with open(testresult_fn, mode='w') as outfile:\n",
        "        outfile = csv.writer(outfile, delimiter=',')\n",
        "        outfile.writerow(['model_fn', 'Accuracy', 'AUC', 'MCC', \n",
        "            'Sensitivity', 'Specificity'])\n",
        "    list_model_fn = sorted(glob.glob(MODEL_DIR+\"/adaptor_*.pkl\"))\n",
        "    \n",
        "    y_prob_mtx = []\n",
        "    arr_labels = []\n",
        "    print(\"==========================TESTING RESULT================================\")\n",
        "    for model_fn in list_model_fn:\n",
        "        print(model_fn)\n",
        "        model.load_state_dict(torch.load(model_fn))\n",
        "        result = model.evaluate(test_loader)\n",
        "        arr_labels = result['arr_labels']\n",
        "        print(\"\\nTest loss: \", result['epoch_loss_avg'])\n",
        "        print(\"Test acc: \", result['acc'])\n",
        "        print(\"Test confusion matrix: \\n\", result['confusion_matrix'])\n",
        "        print(\"Test sensitivity: \", result['sensitivity'])\n",
        "        print(\"Test specificity: \", result['specificity'])\n",
        "        print(\"Test mcc: \", result['mcc'])\n",
        "        print(\"Test auc: \", result['auc'])\n",
        "        \n",
        "        with open(testresult_fn, mode='a') as outfile:\n",
        "            outfile = csv.writer(outfile, delimiter=',')\n",
        "            outfile.writerow([model_fn, result['acc'], result['auc'], \n",
        "                result['mcc'], result['sensitivity'], result['specificity']])\n",
        "        \n",
        "        y_prob_mtx.append(result['arr_prob'])\n",
        "        y_prob_mtx.append(result['arr_prob'])\n",
        "        y_prob_mtx.append(result['arr_prob'])\n",
        "        y_prob_mtx.append(result['arr_prob'])\n",
        "        y_prob_mtx.append(result['arr_prob'])\n",
        "        break\n",
        "\n",
        "    y_prob_mtx = np.array(y_prob_mtx)\n",
        "    print(\"y_prob_mtx: \", y_prob_mtx.shape, \"\\n\", y_prob_mtx)\n",
        "    \n",
        "    y_prob_mean = np.mean(y_prob_mtx, axis=0)    \n",
        "    \n",
        "    auc = metrics.roc_auc_score(arr_labels, y_prob_mean)\n",
        "    print(\"auc: \", auc)\n",
        "    \n",
        "    arr_labels_hyp = []\n",
        "    for prob in y_prob_mean:\n",
        "        if prob > 0.5: arr_labels_hyp.append(1)\n",
        "        else: arr_labels_hyp.append(0)\n",
        "\n",
        "    acc, confusion_matrix, sensitivity, specificity, mcc = model.calculate_confusion_matrix(arr_labels, arr_labels_hyp)\n",
        "        \n",
        "    print(\"Results of Ensemble models: \")\n",
        "    print(\"Test acc: \", acc)\n",
        "    print(\"Test confusion matrix: \\n\", confusion_matrix)\n",
        "    print(\"Test sensitivity: \", sensitivity)\n",
        "    print(\"Test specificity: \", specificity)\n",
        "    print(\"Test mcc: \", mcc)\n",
        "    print(\"Test auc: \", auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isb7mBJo81aH"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Adaptor Training K Fold\\n\")    \n",
        "    lst_path_test_all, lst_label_test_all = create_list_test()        \n",
        "    \n",
        "    #test()\n",
        "    test_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S2trPNZRDhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}